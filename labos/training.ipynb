{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import librosa\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicClassifier(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=input_features, out_features=256, dtype=torch.float32\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=256, out_features=128, dtype=torch.float32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(\n",
    "                in_features=128, out_features=output_features, dtype=torch.float32\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3sec_sample() -> list:\n",
    "    # TODO Vérif\n",
    "    # TODO Rewrite\n",
    "    audio, sample_rate = librosa.load('./Freeze Corleone 667 - Freeze Raël.mp3', sr=None)\n",
    "\n",
    "    segment_duration = 3  # Durée de chaque segment en secondes\n",
    "    segment_length = int(sample_rate * segment_duration)\n",
    "    segments = []\n",
    "\n",
    "    # Effectuez la prédiction toutes les 3 secondes\n",
    "    for i in range(0, len(audio), segment_length):\n",
    "        segment = audio[i : i + segment_length]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_pipeline(audio):\n",
    "    features = []\n",
    "\n",
    "    # Chromagram\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio)\n",
    "    features.append(np.mean(chroma_stft))\n",
    "    features.append(np.var(chroma_stft))  # var => variance\n",
    "\n",
    "    # RMS (Root Mean Square value for each frame)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.var(rms))\n",
    "\n",
    "    # Calcul du Spectral centroid\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio)\n",
    "    features.append(np.mean(spectral_centroids))\n",
    "    features.append(np.var(spectral_centroids))\n",
    "\n",
    "    # Spectral bandwith\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio)\n",
    "    features.append(np.mean(spectral_bandwidth))\n",
    "    features.append(np.var(spectral_bandwidth))\n",
    "\n",
    "    # Calcul du spectral rolloff point\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio)\n",
    "    features.append(np.mean(rolloff))\n",
    "    features.append(np.var(rolloff))\n",
    "\n",
    "    # Calcul du ZCR (Zero Crossing Rate)\n",
    "    zcr = librosa.zero_crossings(audio)\n",
    "    # features.append(np.sum(zcr))  # Custom\n",
    "    features.append(np.mean(zcr))\n",
    "    features.append(np.var(zcr))\n",
    "\n",
    "    # Harmonic\n",
    "    harmony = librosa.effects.harmonic(y=audio)\n",
    "    features.append(np.mean(harmony))\n",
    "    features.append(np.var(harmony))\n",
    "\n",
    "    # Tempo\n",
    "    tempo = librosa.feature.tempo(y=audio)\n",
    "    features.append(tempo[0])\n",
    "\n",
    "    # Calcul des moyennes des MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=audio)\n",
    "    for x in mfcc:\n",
    "        features.append(np.mean(x))\n",
    "        features.append(np.var(x))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  3sec samples extracted and preprocessed\n"
     ]
    }
   ],
   "source": [
    "scaler = joblib.load(\"../resources/standard_scaler_pytorch_model_last.pkl\")\n",
    "\n",
    "dfs = []\n",
    "segments = get_3sec_sample()\n",
    "\n",
    "for audio in segments:\n",
    "    # Perform audio feature extraction\n",
    "    features = audio_pipeline(audio)\n",
    "\n",
    "    # Scale the features using the loaded scaler\n",
    "    scaled_features = scaler.transform([features])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    column_names = [\n",
    "        \"chroma_stft_mean\",\n",
    "        \"chroma_stft_var\",\n",
    "        \"rms_mean\",\n",
    "        \"rms_var\",\n",
    "        \"spectral_centroid_mean\",\n",
    "        \"spectral_centroid_var\",\n",
    "        \"spectral_bandwidth_mean\",\n",
    "        \"spectral_bandwidth_var\",\n",
    "        \"rolloff_mean\",\n",
    "        \"rolloff_var\",\n",
    "        \"zero_crossing_rate_mean\",\n",
    "        \"zero_crossing_rate_var\",\n",
    "        \"harmony_mean\",\n",
    "        \"harmony_var\",\n",
    "        \"tempo\",\n",
    "        \"mfcc1_mean\",\n",
    "        \"mfcc1_var\",\n",
    "        \"mfcc2_mean\",\n",
    "        \"mfcc2_var\",\n",
    "        \"mfcc3_mean\",\n",
    "        \"mfcc3_var\",\n",
    "        \"mfcc4_mean\",\n",
    "        \"mfcc4_var\",\n",
    "        \"mfcc5_mean\",\n",
    "        \"mfcc5_var\",\n",
    "        \"mfcc6_mean\",\n",
    "        \"mfcc6_var\",\n",
    "        \"mfcc7_mean\",\n",
    "        \"mfcc7_var\",\n",
    "        \"mfcc8_mean\",\n",
    "        \"mfcc8_var\",\n",
    "        \"mfcc9_mean\",\n",
    "        \"mfcc9_var\",\n",
    "        \"mfcc10_mean\",\n",
    "        \"mfcc10_var\",\n",
    "        \"mfcc11_mean\",\n",
    "        \"mfcc11_var\",\n",
    "        \"mfcc12_mean\",\n",
    "        \"mfcc12_var\",\n",
    "        \"mfcc13_mean\",\n",
    "        \"mfcc13_var\",\n",
    "        \"mfcc14_mean\",\n",
    "        \"mfcc14_var\",\n",
    "        \"mfcc15_mean\",\n",
    "        \"mfcc15_var\",\n",
    "        \"mfcc16_mean\",\n",
    "        \"mfcc16_var\",\n",
    "        \"mfcc17_mean\",\n",
    "        \"mfcc17_var\",\n",
    "        \"mfcc18_mean\",\n",
    "        \"mfcc18_var\",\n",
    "        \"mfcc19_mean\",\n",
    "        \"mfcc19_var\",\n",
    "        \"mfcc20_mean\",\n",
    "        \"mfcc20_var\",\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(scaled_features, columns=column_names)\n",
    "    dfs.append(df)\n",
    "\n",
    "print(len(dfs), \" 3sec samples extracted and preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping = {\n",
    "    0: \"Blues\",\n",
    "    1: \"Classical\",\n",
    "    2: \"Country\",\n",
    "    3: \"Disco\",\n",
    "    4: \"Hiphop\",\n",
    "    5: \"Jazz\",\n",
    "    6: \"Metal\",\n",
    "    7: \"Pop\",\n",
    "    8: \"Reggae\",\n",
    "    9: \"Rock\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping_inverse = {\n",
    "    \"Blues\": 0,\n",
    "    \"Classical\": 1,\n",
    "    \"Country\": 2,\n",
    "    \"Disco\": 3,\n",
    "    \"Hiphop\": 4,\n",
    "    \"Jazz\": 5,\n",
    "    \"Metal\": 6,\n",
    "    \"Pop\": 7,\n",
    "    \"Reggae\": 8,\n",
    "    \"Rock\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "my_model = MusicClassifier(input_features=55, output_features=10)\n",
    "my_model.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"../resources/actual_model_fast.pth\", map_location=torch.device(\"cpu\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jazz 12\n",
      "Classical 18\n",
      "Blues 25\n",
      "Reggae 2\n",
      "Country 11\n",
      "Hiphop 36\n",
      "Metal 1\n",
      "Results => Hiphop ( 4 )\n"
     ]
    }
   ],
   "source": [
    "my_model.eval()\n",
    "\n",
    "class_predictions = []\n",
    "raw_results = []\n",
    "\n",
    "for df in dfs:\n",
    "    y_logits = my_model(torch.from_numpy(df.to_numpy()).type(torch.float32))\n",
    "    y_softmax = torch.softmax(y_logits, dim=1)\n",
    "    y_pred = y_softmax.argmax(dim=1)\n",
    "\n",
    "    # print(genre_mapping[y_pred.detach().numpy()[0]])\n",
    "    # print(list(torch.round(y_softmax * 1000) / 1000))\n",
    "\n",
    "    raw_results.append(y_softmax.detach().numpy())\n",
    "    class_predictions.append(genre_mapping[y_pred.detach().numpy()[0]])\n",
    "\n",
    "unique_values = set(class_predictions)\n",
    "actual_best = 0\n",
    "for elt in unique_values:\n",
    "    if class_predictions.count(elt) > actual_best:\n",
    "        actual_best = class_predictions.count(elt)\n",
    "        prediction = elt\n",
    "    print(elt, class_predictions.count(elt))\n",
    "\n",
    "print(\"Results =>\", prediction, \"(\", genre_mapping_inverse[prediction], \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_length => 51\n"
     ]
    }
   ],
   "source": [
    "real_class = 4 # ! Flexibiliser\n",
    "kept_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    sorted_array = np.sort(raw_results[i][0])[::-1]\n",
    "    # Keep only dfs when the real class i the first or second predicted by the model !\n",
    "    if raw_results[i][0][real_class] == sorted_array[0] or (raw_results[i][0][real_class] == sorted_array[1] and raw_results[i][0][real_class] > 0):\n",
    "        # print(raw_results[i][0][real_class])\n",
    "        kept_dfs.append(dfs[i])\n",
    "\n",
    "print(\"df_length =>\",len(kept_dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grow dataset with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(dfs: List[pd.DataFrame], real_class):\n",
    "    # Creating new df from processed input\n",
    "    for i in range(len(dfs)):\n",
    "        if i == 0:\n",
    "            new_df = pd.DataFrame(dfs[i], columns=column_names)\n",
    "        else:\n",
    "            new_df = pd.concat([new_df, dfs[i]], axis=0)\n",
    "    new_df[\"label\"] = real_class\n",
    "\n",
    "    # Créer dataset enrichie (csv)\n",
    "    original_df = pd.read_csv(\n",
    "        \"../resources/original_dataset.csv\"\n",
    "    )  # ! Fait ça teh l'afaire\n",
    "    # TODO Change to actual\n",
    "\n",
    "    concatened_dataset = pd.concat([original_df, new_df], axis=0)\n",
    "\n",
    "    concatened_dataset.to_csv(\"./actual_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dfs(kept_dfs, real_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(MusicClassifier):\n",
    "    # TODO: Externalise this\n",
    "    # Init le model\n",
    "    torch.manual_seed(42)\n",
    "    model = MusicClassifier(input_features=55, output_features=10)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.011)\n",
    "\n",
    "    def accuracy_fn(y_true, y_pred):\n",
    "        correct = (\n",
    "            torch.eq(input=y_true, other=y_pred).sum().item()\n",
    "        )  # torch.eq() calculates where two tensors are equal\n",
    "        acc = (correct / len(y_pred)) * 100  # Calcul simple de pourcentage\n",
    "        return acc\n",
    "\n",
    "    # Prepare data\n",
    "    df = pd.read_csv(\"../resources/actual_dataset.csv\")\n",
    "    # df = pd.read_csv(\"/app/resources/original_dataset.csv\")\n",
    "    X = torch.from_numpy(df.drop(columns=[\"label\"]).to_numpy()).type(torch.float32)\n",
    "    y = torch.from_numpy(df[\"label\"].to_numpy()).type(torch.long)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    torch.manual_seed(42)\n",
    "    epochs = 125\n",
    "    for epoch in range(epochs + 1):\n",
    "        \"\"\"\n",
    "        Train\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_logits = model(X_train)\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        # 2. Metrics\n",
    "        loss = loss_fn(y_logits, y_train)\n",
    "        acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        # 2.1 Save metrics\n",
    "        # loss_history.append(loss.cpu().detach().numpy())\n",
    "        # acc_history.append(acc)\n",
    "\n",
    "        # 3. Zero Grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimmizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Test\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            y_test_logits = model(X_test)\n",
    "            y_test_pred = torch.softmax(y_test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "            # 2. Metrics\n",
    "            test_loss = loss_fn(y_test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_pred=y_test_pred, y_true=y_test)\n",
    "\n",
    "            # 2.1 Save metrics\n",
    "            # test_loss_history.append(test_loss.cpu().detach().numpy())\n",
    "            # test_acc_history.append(test_acc)\n",
    "\n",
    "        # Print out what's happening\n",
    "        if epoch % 25 == 0:\n",
    "                print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.29285, Acc: 12.11% | Test Loss: 1.88716, Test Acc: 37.39%\n",
      "Epoch: 25 | Loss: 0.60037, Acc: 78.96% | Test Loss: 0.65277, Test Acc: 77.22%\n",
      "Epoch: 50 | Loss: 0.31458, Acc: 88.92% | Test Loss: 0.45170, Test Acc: 85.14%\n",
      "Epoch: 75 | Loss: 0.16288, Acc: 94.48% | Test Loss: 0.39714, Test Acc: 88.16%\n",
      "Epoch: 100 | Loss: 0.10911, Acc: 96.19% | Test Loss: 0.38385, Test Acc: 90.14%\n",
      "Epoch: 125 | Loss: 0.07611, Acc: 97.63% | Test Loss: 0.36349, Test Acc: 90.69%\n"
     ]
    }
   ],
   "source": [
    "training_loop(MusicClassifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
