{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import librosa\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicClassifier(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=input_features, out_features=256, dtype=torch.float32\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=256, out_features=128, dtype=torch.float32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(\n",
    "                in_features=128, out_features=output_features, dtype=torch.float32\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3sec_sample() -> list:\n",
    "    # TODO Vérif\n",
    "    # TODO Rewrite\n",
    "    audio, sample_rate = librosa.load('./Freeze Corleone 667 - Freeze Raël.mp3', sr=None)\n",
    "\n",
    "    segment_duration = 3  # Durée de chaque segment en secondes\n",
    "    segment_length = int(sample_rate * segment_duration)\n",
    "    segments = []\n",
    "\n",
    "    # Effectuez la prédiction toutes les 3 secondes\n",
    "    for i in range(0, len(audio), segment_length):\n",
    "        segment = audio[i : i + segment_length]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_pipeline(audio):\n",
    "    features = []\n",
    "\n",
    "    # Chromagram\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio)\n",
    "    features.append(np.mean(chroma_stft))\n",
    "    features.append(np.var(chroma_stft))  # var => variance\n",
    "\n",
    "    # RMS (Root Mean Square value for each frame)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.var(rms))\n",
    "\n",
    "    # Calcul du Spectral centroid\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio)\n",
    "    features.append(np.mean(spectral_centroids))\n",
    "    features.append(np.var(spectral_centroids))\n",
    "\n",
    "    # Spectral bandwith\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio)\n",
    "    features.append(np.mean(spectral_bandwidth))\n",
    "    features.append(np.var(spectral_bandwidth))\n",
    "\n",
    "    # Calcul du spectral rolloff point\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio)\n",
    "    features.append(np.mean(rolloff))\n",
    "    features.append(np.var(rolloff))\n",
    "\n",
    "    # Calcul du ZCR (Zero Crossing Rate)\n",
    "    zcr = librosa.zero_crossings(audio)\n",
    "    # features.append(np.sum(zcr))  # Custom\n",
    "    features.append(np.mean(zcr))\n",
    "    features.append(np.var(zcr))\n",
    "\n",
    "    # Harmonic\n",
    "    harmony = librosa.effects.harmonic(y=audio)\n",
    "    features.append(np.mean(harmony))\n",
    "    features.append(np.var(harmony))\n",
    "\n",
    "    # Tempo\n",
    "    tempo = librosa.feature.tempo(y=audio)\n",
    "    features.append(tempo[0])\n",
    "\n",
    "    # Calcul des moyennes des MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=audio)\n",
    "    for x in mfcc:\n",
    "        features.append(np.mean(x))\n",
    "        features.append(np.var(x))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/home/payetquentin/Documents/workspace/briefs/S9_music/Music/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  3sec samples extracted and preprocessed\n"
     ]
    }
   ],
   "source": [
    "scaler = joblib.load(\"../resources/standard_scaler_pytorch_model_last.pkl\")\n",
    "\n",
    "dfs = []\n",
    "segments = get_3sec_sample()\n",
    "\n",
    "for audio in segments:\n",
    "    # Perform audio feature extraction\n",
    "    features = audio_pipeline(audio)\n",
    "\n",
    "    # Scale the features using the loaded scaler\n",
    "    scaled_features = scaler.transform([features])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    column_names = [\n",
    "        \"chroma_stft_mean\",\n",
    "        \"chroma_stft_var\",\n",
    "        \"rms_mean\",\n",
    "        \"rms_var\",\n",
    "        \"spectral_centroid_mean\",\n",
    "        \"spectral_centroid_var\",\n",
    "        \"spectral_bandwidth_mean\",\n",
    "        \"spectral_bandwidth_var\",\n",
    "        \"rolloff_mean\",\n",
    "        \"rolloff_var\",\n",
    "        \"zero_crossing_rate_mean\",\n",
    "        \"zero_crossing_rate_var\",\n",
    "        \"harmony_mean\",\n",
    "        \"harmony_var\",\n",
    "        \"tempo\",\n",
    "        \"mfcc1_mean\",\n",
    "        \"mfcc1_var\",\n",
    "        \"mfcc2_mean\",\n",
    "        \"mfcc2_var\",\n",
    "        \"mfcc3_mean\",\n",
    "        \"mfcc3_var\",\n",
    "        \"mfcc4_mean\",\n",
    "        \"mfcc4_var\",\n",
    "        \"mfcc5_mean\",\n",
    "        \"mfcc5_var\",\n",
    "        \"mfcc6_mean\",\n",
    "        \"mfcc6_var\",\n",
    "        \"mfcc7_mean\",\n",
    "        \"mfcc7_var\",\n",
    "        \"mfcc8_mean\",\n",
    "        \"mfcc8_var\",\n",
    "        \"mfcc9_mean\",\n",
    "        \"mfcc9_var\",\n",
    "        \"mfcc10_mean\",\n",
    "        \"mfcc10_var\",\n",
    "        \"mfcc11_mean\",\n",
    "        \"mfcc11_var\",\n",
    "        \"mfcc12_mean\",\n",
    "        \"mfcc12_var\",\n",
    "        \"mfcc13_mean\",\n",
    "        \"mfcc13_var\",\n",
    "        \"mfcc14_mean\",\n",
    "        \"mfcc14_var\",\n",
    "        \"mfcc15_mean\",\n",
    "        \"mfcc15_var\",\n",
    "        \"mfcc16_mean\",\n",
    "        \"mfcc16_var\",\n",
    "        \"mfcc17_mean\",\n",
    "        \"mfcc17_var\",\n",
    "        \"mfcc18_mean\",\n",
    "        \"mfcc18_var\",\n",
    "        \"mfcc19_mean\",\n",
    "        \"mfcc19_var\",\n",
    "        \"mfcc20_mean\",\n",
    "        \"mfcc20_var\",\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(scaled_features, columns=column_names)\n",
    "    dfs.append(df)\n",
    "\n",
    "print(len(dfs), \" 3sec samples extracted and preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping = {\n",
    "    0: \"Blues\",\n",
    "    1: \"Classical\",\n",
    "    2: \"Country\",\n",
    "    3: \"Disco\",\n",
    "    4: \"Hiphop\",\n",
    "    5: \"Jazz\",\n",
    "    6: \"Metal\",\n",
    "    7: \"Pop\",\n",
    "    8: \"Reggae\",\n",
    "    9: \"Rock\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping_inverse = {\n",
    "    \"Blues\": 0,\n",
    "    \"Classical\": 1,\n",
    "    \"Country\": 2,\n",
    "    \"Disco\": 3,\n",
    "    \"Hiphop\": 4,\n",
    "    \"Jazz\": 5,\n",
    "    \"Metal\": 6,\n",
    "    \"Pop\": 7,\n",
    "    \"Reggae\": 8,\n",
    "    \"Rock\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "my_model = MusicClassifier(input_features=55, output_features=10)\n",
    "my_model.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"../resources/actual_model_fast.pth\", map_location=torch.device(\"cpu\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jazz 12\n",
      "Classical 18\n",
      "Blues 25\n",
      "Reggae 2\n",
      "Country 11\n",
      "Hiphop 36\n",
      "Metal 1\n",
      "Results => Hiphop ( 4 )\n"
     ]
    }
   ],
   "source": [
    "my_model.eval()\n",
    "\n",
    "class_predictions = []\n",
    "raw_results = []\n",
    "\n",
    "for df in dfs:\n",
    "    y_logits = my_model(torch.from_numpy(df.to_numpy()).type(torch.float32))\n",
    "    y_softmax = torch.softmax(y_logits, dim=1)\n",
    "    y_pred = y_softmax.argmax(dim=1)\n",
    "\n",
    "    # print(genre_mapping[y_pred.detach().numpy()[0]])\n",
    "    # print(list(torch.round(y_softmax * 1000) / 1000))\n",
    "\n",
    "    raw_results.append(y_softmax.detach().numpy())\n",
    "    class_predictions.append(genre_mapping[y_pred.detach().numpy()[0]])\n",
    "\n",
    "unique_values = set(class_predictions)\n",
    "actual_best = 0\n",
    "for elt in unique_values:\n",
    "    if class_predictions.count(elt) > actual_best:\n",
    "        actual_best = class_predictions.count(elt)\n",
    "        prediction = elt\n",
    "    print(elt, class_predictions.count(elt))\n",
    "\n",
    "print(\"Results =>\", prediction, \"(\", genre_mapping_inverse[prediction], \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_length => 51\n"
     ]
    }
   ],
   "source": [
    "real_class = 4\n",
    "kept_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    sorted_array = np.sort(raw_results[i][0])[::-1]\n",
    "    # Keep only dfs when the real class i the first or second predicted by the model !\n",
    "    if raw_results[i][0][real_class] == sorted_array[0] or (raw_results[i][0][real_class] == sorted_array[1] and raw_results[i][0][real_class] > 0):\n",
    "        # print(raw_results[i][0][real_class])\n",
    "        kept_dfs.append(dfs[i])\n",
    "\n",
    "print(\"df_length =>\",len(kept_dfs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
